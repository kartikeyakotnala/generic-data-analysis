import pandas as pd
import matplotlib.pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.decomposition import TruncatedSVD
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Step 1: Read CSV file
dir = 'path_to_your_directory'  # Update this with the actual directory path
file_path = f'{dir}/your_file.csv'  # Update this with your file name
data = pd.read_csv(file_path)

# Step 2: Separate numerical and nominal variables excluding label column
label_column = data.columns[-1]
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns.difference([label_column])
nominal_columns = data.select_dtypes(include=['object']).columns.difference([label_column])

# Step 3: Compare numerical variables with the label and plot graphs
for num_column in numerical_columns:
    plt.style.use('dark_background')
    plt.scatter(data[num_column], data[label_column], alpha=0.5)
    plt.title(f'{num_column} vs {label_column}')
    plt.xlabel(num_column)
    plt.ylabel(label_column)
    plt.show()

# Step 4: Create a pipeline for preprocessing
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_columns),
        ('cat', categorical_transformer, nominal_columns)
    ])

# Step 5: Use TruncatedSVD to filter out unnecessary data features
# Let's assume you want to keep 95% of the variance
truncated_svd = TruncatedSVD(n_components=X.shape[1])  # Set n_components to the number of features

# Step 6: Create a model pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('truncated_svd', truncated_svd),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Step 7: Split data into features (X) and target (y), and then into training and testing sets
X = data.drop(columns=[label_column])
y = data[label_column]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit TruncatedSVD on the training data
X_train_transformed = truncated_svd.fit_transform(X_train)

# Now, find the number of components to explain 95% of the variance
cumulative_variance_ratio = truncated_svd.explained_variance_ratio_.cumsum()
n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1

# Update TruncatedSVD with the determined number of components
truncated_svd = TruncatedSVD(n_components=n_components)

# Update the model pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('truncated_svd', truncated_svd),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Step 8: Fit the model and make predictions
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Step 9: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Model Accuracy: {accuracy * 100:.2f}%')
